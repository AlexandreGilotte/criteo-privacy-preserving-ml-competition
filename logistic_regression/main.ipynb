{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516b5db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:52:00.743189Z",
     "start_time": "2021-09-17T12:51:43.263958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Common libs to import everywhere\n",
    "import gc\n",
    "import numba\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c90a0",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "- reading one-hot encoded data, generated by the notebook AggLogistic-EncodeData.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197c2be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:52:00.748467Z",
     "start_time": "2021-09-17T12:52:00.745958Z"
    }
   },
   "outputs": [],
   "source": [
    "name = f\"../data/encodedAggData_C_D_S_X_Xbis_n_pairs.npz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8360c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:52:42.752941Z",
     "start_time": "2021-09-17T12:52:00.750768Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(name)\n",
    "C = data[\"C\"]\n",
    "D = data[\"D\"]\n",
    "S = data[\"S\"]\n",
    "X_test = data[\"X_test\"]\n",
    "X_another_set = data[\"X_another_set\"]\n",
    "nb_samples_agg = data[\"nb_samples_agg\"]\n",
    "pairs = data[\"pairs\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b43e27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:53:39.328994Z",
     "start_time": "2021-09-17T12:52:42.756293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Labels  (Used for computing validation score, not used for training!)\n",
    "\n",
    "Y_clicks_test = np.loadtxt(\"../data/y_test.csv\", skiprows=1, delimiter=\",\", dtype=np.int32, usecols=(0,))\n",
    "Y_sales_test = np.loadtxt(\"../data/y_test.csv\", skiprows=1, delimiter=\",\", dtype=np.int32, usecols=(1,))\n",
    "\n",
    "Y_clicks_another_set = np.loadtxt(\n",
    "    \"../data/criteo-ppml-challenge-adkdd21-dataset-additional-test-data.csv\",\n",
    "    skiprows=1,\n",
    "    delimiter=\",\",\n",
    "    dtype=np.int32,\n",
    "    usecols=(19,),\n",
    ")\n",
    "Y_sales_another_set = np.loadtxt(\n",
    "    \"../data/criteo-ppml-challenge-adkdd21-dataset-additional-test-data.csv\",\n",
    "    skiprows=1,\n",
    "    delimiter=\",\",\n",
    "    dtype=np.int32,\n",
    "    usecols=(20,),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96af1bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:53:39.337976Z",
     "start_time": "2021-09-17T12:53:39.331618Z"
    }
   },
   "outputs": [],
   "source": [
    "run_on_sales = True\n",
    "verbose = False\n",
    "logfile = f\"results_agglogistic_{'sales' if run_on_sales else 'clicks'}.log\"\n",
    "logfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c056b04",
   "metadata": {},
   "source": [
    "# Training code\n",
    "- logistic regresion model sigmoid( w.x + b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c78219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:53:39.344083Z",
     "start_time": "2021-09-17T12:53:39.341005Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_and_log(x):\n",
    "    print(x)\n",
    "    with open(logfile, \"a+\") as handle:\n",
    "        handle.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96efd69b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:53:39.373397Z",
     "start_time": "2021-09-17T12:53:39.346521Z"
    }
   },
   "outputs": [],
   "source": [
    "def logit(x):\n",
    "    return -np.log((1 / x) - 1)\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def predict(w, bias, x):\n",
    "    # bias = -2.2  ##  hardcoded bias, ~=  logit( nbclick/ nb displays  )\n",
    "    return sigmoid(w[x].sum() + bias)\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def predicts(w, bias, xs):\n",
    "    return np.array([predict(w, bias, x) for x in xs])\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def project(X, y, n):\n",
    "    p = np.zeros(n)\n",
    "    len_y = len(y)\n",
    "    for i in range(0, len_y):\n",
    "        xi = X[i]\n",
    "        yi = y[i]\n",
    "        for h in xi:\n",
    "            p[h] += yi\n",
    "    return p\n",
    "\n",
    "\n",
    "def LLH(prediction, y):\n",
    "    llh = np.log(prediction) * y + np.log(1 - prediction) * (1 - y)\n",
    "    return sum(llh) / len(y)\n",
    "\n",
    "\n",
    "def Entropy(y):\n",
    "    py = sum(y > 0) / len(y)\n",
    "    return py * np.log(py) + (1 - py) * np.log(1 - py)\n",
    "\n",
    "\n",
    "def NLlh_(prediction, y):\n",
    "    if any(prediction <= 0) or any(prediction >= 1):\n",
    "        return np.nan\n",
    "    h = Entropy(y)\n",
    "    llh = LLH(prediction, y)\n",
    "    return (h - llh) / h\n",
    "\n",
    "\n",
    "def Nllh(w, bias, X, y):\n",
    "    preds = predicts(w, bias, X)\n",
    "    return NLlh_(preds, y)\n",
    "\n",
    "\n",
    "l2 = 500\n",
    "\n",
    "\n",
    "def gradient(w, pC, C, l2, D=None, count_agg_X=None):\n",
    "\n",
    "    pc_corrected = pC\n",
    "    c_corrected = C\n",
    "\n",
    "    if count_agg_X is not None:\n",
    "        # rescaling clicks or predicted clicks by the ratio between displays in aggregated data\n",
    "        # and displays in (re-aggregated) granular dataset X\n",
    "        pc_corrected = pC * np.minimum(D / (count_agg_X + 1), 1)\n",
    "        c_corrected = C * np.minimum(count_agg_X / (D + 1), 1)\n",
    "\n",
    "    g = pc_corrected - c_corrected\n",
    "\n",
    "    g += l2 * w  ## regularization term 1/2 * l2 * sum(w*w)\n",
    "    g[0] = 0  ##  coordinate 0 is 'missing' modality, shared between all pairs. not trying to learn it.\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "# Gradient rescaled by inverse diagonal Hessian\n",
    "def descent_direction(w, bias, X, C, global_scale_factor, D, count_agg_X, l2):\n",
    "    preds = predicts(w, bias, X)\n",
    "    pC = project(X, preds, len(w)) * global_scale_factor\n",
    "    g = gradient(w, pC, C, l2, D, count_agg_X)\n",
    "    h = pC + l2 \n",
    "    dir_ = -g / h\n",
    "    return dir_\n",
    "\n",
    "\n",
    "verbose = False\n",
    "\n",
    "\n",
    "def train(\n",
    "    bias,\n",
    "    X,\n",
    "    C,\n",
    "    scale_factor,\n",
    "    D,\n",
    "    count_agg_X,\n",
    "    l2,\n",
    "    alpha=0.01,\n",
    "    maxiters=100,\n",
    "    X_validation=None,\n",
    "    Y_validation=None,  # to monitor progress during learning\n",
    "    w=None,\n",
    "):\n",
    "    ## Init model\n",
    "    w = np.zeros(len(C)) if w is None else w\n",
    "    nbiters = 0\n",
    "    for i in range(0, maxiters):\n",
    "        nbiters += 1\n",
    "        direction = descent_direction(w, bias, X, C, scale_factor, D, count_agg_X, l2)\n",
    "        w += alpha * direction\n",
    "        if verbose and X_validation is not None:\n",
    "            nllh = Nllh(w, bias, X_validation, Y_validation)\n",
    "            print(f\"{nbiters}  --  {nllh:.4f}    \", end=\"\\r\")\n",
    "            if 9 == i % 10:\n",
    "                print(\"\")\n",
    "    return w, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ceac07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:53:43.459722Z",
     "start_time": "2021-09-17T12:53:39.375230Z"
    }
   },
   "outputs": [],
   "source": [
    "## Compiling numba\n",
    "n = len(D)\n",
    "w = np.zeros(n)\n",
    "\n",
    "preds = predicts(w, -2.2, X_test[:10, :])\n",
    "p = project(X_test[:10, :], preds, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28d25a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:53:43.476230Z",
     "start_time": "2021-09-17T12:53:43.462618Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(\n",
    "    D,\n",
    "    C,\n",
    "    nb_samples_agg,  # aggregated data\n",
    "    gaussian_sigma,  # Stdev of noise which should be added to agg data\n",
    "    X_train,\n",
    "    X_validation,\n",
    "    Y_validation,  # to monitor training\n",
    "    l2,  # L2 regularization weight\n",
    "    use_by_coordinate_rescaling,\n",
    "    min_displays=None,  # remove crossfeatures with less than minDisplays\n",
    "    alpha=0.01,\n",
    "    maxiters=100,  # optimizer stepsize and nb iters\n",
    "    w=None,\n",
    "):\n",
    "\n",
    "    nb_samples_not_agg = len(X_train)\n",
    "    scale_factor = nb_samples_agg / nb_samples_not_agg\n",
    "    # print(\"scaleFactor\", scaleFactor)\n",
    "\n",
    "    count_agg_X = None\n",
    "    if use_by_coordinate_rescaling:\n",
    "        # aggregated number of displays; computed on X_test, rescaled to size of agg dataset.\n",
    "        count_agg_X = project(X_train, np.ones(X_train.shape[0]), n) * scale_factor\n",
    "\n",
    "    D = D.copy()\n",
    "    C = C.copy()\n",
    "\n",
    "    ## Adding noise where there is data ( Not truly diff private, but similar to the challenge)\n",
    "    if gaussian_sigma > 0:\n",
    "        D += np.random.normal(0, gaussian_sigma, len(D))\n",
    "        C += np.random.normal(0, gaussian_sigma, len(D))\n",
    "\n",
    "        D[0] = 0  #  \"other\" modality\n",
    "        C[0] = 0\n",
    "\n",
    "    if min_displays is not None:\n",
    "        ind = np.where(D < min_displays)[0]\n",
    "        D[ind] = 0\n",
    "        C[ind] = 0\n",
    "        # print( \"Proportion of crosses set to 0\"  ,  len(ind) / len(D) )\n",
    "\n",
    "    ## removing absurd values (which might be observed because of the noise)\n",
    "    D = np.maximum(D, 0)\n",
    "    C = np.maximum(C, 0)\n",
    "    C = np.minimum(C, D)\n",
    "\n",
    "    bias = logit(C.sum() / D.sum())  # hardcode bias, I did not implement the gradient on the bias\n",
    "\n",
    "    # print( \"sigma\",gaussianSigma,  \"l2\",l2 , \"minDisplays\", minDisplays,\n",
    "    #       \"scaleFactor\",scaleFactor ,\n",
    "    #      \"useRescaling\", CountAggX is not None, \"noiseModelScaling\",noiseModelScaling  )\n",
    "    w, bias = train(\n",
    "        bias,\n",
    "        X_train,\n",
    "        C,\n",
    "        scale_factor,\n",
    "        D,\n",
    "        count_agg_X,\n",
    "        l2=l2,\n",
    "        alpha=alpha,\n",
    "        maxiters=maxiters,\n",
    "        X_validation=X_validation,\n",
    "        Y_validation=Y_validation,\n",
    "        w=w,\n",
    "    )\n",
    "    return w, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534ebd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:53:43.486247Z",
     "start_time": "2021-09-17T12:53:43.478234Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_(\n",
    "    nb_train_samples,\n",
    "    train_with_X_test=False,\n",
    "    gaussian_sigma=0,  # noise level\n",
    "    l2=100,  # L2 regularization weight\n",
    "    use_by_coordinate_rescaling=True,\n",
    "    alpha=0.01,\n",
    "    maxiters=100,\n",
    "):\n",
    "\n",
    "    if not run_on_sales:  ## Global param\n",
    "        agg_labels = C\n",
    "        Y_test = Y_clicks_test\n",
    "        Y_another_set = Y_clicks_another_set\n",
    "\n",
    "    else:\n",
    "        agg_labels = S\n",
    "        Y_test = Y_sales_test\n",
    "        Y_another_set = Y_sales_another_set\n",
    "\n",
    "    X_validation = X_another_set[-1_000_000:, :]\n",
    "    Y_validation = Y_another_set[-1_000_000:]\n",
    "\n",
    "    X_train = X_test if train_with_X_test else X_another_set\n",
    "    ind = np.random.permutation(X_train.shape[0])[:nb_train_samples]\n",
    "    X_train = X_train[ind, :]\n",
    "\n",
    "    w, bias = run(\n",
    "        D,\n",
    "        agg_labels,\n",
    "        nb_samples_agg,  # aggregated data\n",
    "        gaussian_sigma,  # noise level\n",
    "        X_train,  # note that Ytrain is only used to compute score during training. Works with Ytrain = None\n",
    "        X_validation,\n",
    "        Y_validation,  # to monitor training && pick parameters\n",
    "        l2=l2,  # L2 regularization weight\n",
    "        use_by_coordinate_rescaling=use_by_coordinate_rescaling,\n",
    "        min_displays=None,  # remove crossfeatures with less than minDisplays\n",
    "        alpha=alpha,\n",
    "        maxiters=maxiters,\n",
    "    )\n",
    "\n",
    "    llh_valid = Nllh(w, bias, X_validation, Y_validation)\n",
    "    llh_test = Nllh(w, bias, X_test, Y_test)\n",
    "\n",
    "    tolog = (\n",
    "        f\"run_on_sales:{run_on_sales}; llh_valid:{llh_valid};llh_test:{llh_test}; nb_train_samples:{nb_train_samples};\"\n",
    "        + f\"gaussian_sigma:{gaussian_sigma};train_with_X_test:{train_with_X_test};\"\n",
    "        + f\"l2:{l2};use_by_coordinate_rescaling:{use_by_coordinate_rescaling};\"\n",
    "        + f\"alpha:{alpha};maxiters:{maxiters}\"\n",
    "    )\n",
    "    print_and_log(tolog)\n",
    "\n",
    "    return w, bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19c108",
   "metadata": {},
   "source": [
    "# Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96e07c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:53:43.491299Z",
     "start_time": "2021-09-17T12:53:43.487870Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_train_samples = len(X_test)\n",
    "print(nb_train_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae6f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T12:58:49.688997Z",
     "start_time": "2021-09-17T12:53:43.492872Z"
    }
   },
   "outputs": [],
   "source": [
    "default_l2 = 1_000\n",
    "w, bias = run_(\n",
    "    nb_train_samples,\n",
    "    train_with_X_test=True,\n",
    "    gaussian_sigma=0,  # noise level\n",
    "    l2=default_l2,  # L2 regularization weight\n",
    "    use_by_coordinate_rescaling=True,\n",
    "    alpha=0.01,\n",
    "    maxiters=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61c77c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T13:53:48.645613Z",
     "start_time": "2021-09-17T12:58:49.691557Z"
    }
   },
   "outputs": [],
   "source": [
    "print_and_log(\"Benching L2\")\n",
    "for l2 in [25, 50, 100, 200, 400, 800, 1000, 2000, 4000, 8000, 16000]:\n",
    "    w, bias = run_(\n",
    "        nb_train_samples,\n",
    "        train_with_X_test=True,\n",
    "        gaussian_sigma=0,  # noise level\n",
    "        l2=l2,  # L2 regularization weight\n",
    "        use_by_coordinate_rescaling=True,\n",
    "        alpha=0.01,\n",
    "        maxiters=100,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0be392",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-17T12:51:43.232Z"
    }
   },
   "outputs": [],
   "source": [
    "print_and_log(\"Benching L2 with train_with_X_test = False\")\n",
    "for l2 in [25, 50, 100, 200, 400, 800, 1000, 2000, 4000, 8000, 16000]:\n",
    "    w, bias = run_(\n",
    "        nb_train_samples,\n",
    "        train_with_X_test=False,\n",
    "        gaussian_sigma=0,  # noise level\n",
    "        l2=l2,  # L2 regularization weight\n",
    "        use_by_coordinate_rescaling=True,\n",
    "        alpha=0.01,\n",
    "        maxiters=100,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b091d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-17T12:51:43.236Z"
    }
   },
   "outputs": [],
   "source": [
    "print_and_log(\"Benching alpha\")\n",
    "\n",
    "for alpha in [0.01, 0.05, 0.01 / 5]:\n",
    "    w, bias = run_(\n",
    "        nb_train_samples,\n",
    "        train_with_X_test=True,\n",
    "        gaussian_sigma=0,  # noise level\n",
    "        l2=default_l2,  # L2 regularization weight\n",
    "        use_by_coordinate_rescaling=True,\n",
    "        alpha=alpha,\n",
    "        maxiters=100,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d9412",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-17T12:51:43.238Z"
    }
   },
   "outputs": [],
   "source": [
    "print_and_log(\"Benching maxiter\")\n",
    "\n",
    "for maxiters in [20, 50, 100, 200]:\n",
    "    w, bias = run_(\n",
    "        nb_train_samples,\n",
    "        train_with_X_test=True,\n",
    "        gaussian_sigma=0,  # noise level\n",
    "        l2=default_l2,  # L2 regularization weight\n",
    "        use_by_coordinate_rescaling=True,\n",
    "        alpha=0.01,\n",
    "        maxiters=maxiters,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec512a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-17T12:51:43.246Z"
    }
   },
   "outputs": [],
   "source": [
    "## no rescaling\n",
    "print_and_log(\"No rescaling, benching l2\")\n",
    "\n",
    "for l2 in [25, 50, 100, 200, 400, 800, 1000, 2000, 4000, 8000, 16000]:\n",
    "    w, bias = run_(\n",
    "        nb_train_samples,\n",
    "        train_with_X_test=True,\n",
    "        gaussian_sigma=0,  # noise level\n",
    "        l2=l2,  # L2 regularization weight\n",
    "        use_by_coordinate_rescaling=False,\n",
    "        alpha=0.01,\n",
    "        maxiters=100,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe76842",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-17T12:51:43.251Z"
    }
   },
   "outputs": [],
   "source": [
    "## trainWithXTest = False, changing nbsamples\n",
    "print_and_log(\"trainWithXTest = False, changing nb_train_samples\")\n",
    "\n",
    "for nb_samples in [10_000, 20_000, 50_000, 100_000, 200_000, 500_000, 1_000_000, 2_000_000, X_another_set.shape[0]]:\n",
    "    for i in range(0, 5):\n",
    "        gc.collect()\n",
    "        w, bias = run_(\n",
    "            nb_samples,\n",
    "            train_with_X_test=False,\n",
    "            gaussian_sigma=0,  # noise level\n",
    "            l2=default_l2,  # L2 regularization weight\n",
    "            use_by_coordinate_rescaling=True,\n",
    "            alpha=0.01,\n",
    "            maxiters=100,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294bc1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-17T12:51:43.254Z"
    }
   },
   "outputs": [],
   "source": [
    "## With noise; and quick l2 bench\n",
    "print_and_log(\"With noise\")\n",
    "\n",
    "for sigma in [10, 17, 50, 250, 1_000, 5_000, 25_000, 100_000]:\n",
    "    for l2 in [1000, 4000, 16_000, 64_000]:\n",
    "        for i in range(0, 5):\n",
    "            gc.collect()\n",
    "            w, bias = run_(\n",
    "                nb_train_samples,\n",
    "                train_with_X_test=True,\n",
    "                gaussian_sigma=sigma,  # noise level\n",
    "                l2=default_l2,  # L2 regularization weight\n",
    "                use_by_coordinate_rescaling=True,\n",
    "                alpha=0.01,\n",
    "                maxiters=100,\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "a36b085e9ff735ee0fad94699c928a54e5528d32a02652250964e3ed6cb11995"
  },
  "kernelspec": {
   "display_name": "aggregate_models_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
