{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6cf9f3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23732cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:02:58.769661Z",
     "start_time": "2022-01-14T18:02:57.785146Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bac87d",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cc362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:03:11.118181Z",
     "start_time": "2022-01-14T18:02:58.771898Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "X_TRAIN_FN = \"X_train.csv\"\n",
    "Y_TRAIN_FN = \"y_train.csv\"\n",
    "X_TEST_FN = \"X_test.csv\"\n",
    "Y_TEST_FN = \"y_test.csv\"\n",
    "LARGE_TEST_FN = \"criteo-ppml-challenge-adkdd21-dataset-additional-test-data.csv\"\n",
    "SINGLE_AGG_FN = \"aggregated_singles.csv\"\n",
    "DOUBLE_AGG_FN = \"aggregated_pairs.csv\"\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    X_train_path = os.path.join(os.path.join(DATA_DIR, X_TRAIN_FN))\n",
    "    y_train_path = os.path.join(os.path.join(DATA_DIR, Y_TRAIN_FN))\n",
    "    X_test_path = os.path.join(os.path.join(DATA_DIR, X_TEST_FN))\n",
    "    y_test_path = os.path.join(os.path.join(DATA_DIR, Y_TEST_FN))\n",
    "    single_agg_path = os.path.join(os.path.join(DATA_DIR, SINGLE_AGG_FN))\n",
    "    double_agg_path = os.path.join(os.path.join(DATA_DIR, DOUBLE_AGG_FN))\n",
    "    large_test_path = os.path.join(os.path.join(DATA_DIR, LARGE_TEST_FN))\n",
    "\n",
    "    X = pd.read_csv(X_train_path)\n",
    "    y = pd.read_csv(y_train_path).click\n",
    "    X_test = pd.read_csv(X_test_path)\n",
    "    y_test = pd.read_csv(y_test_path).click\n",
    "    df_single_agg = pd.read_csv(single_agg_path, index_col=0)\n",
    "    df_double_agg = pd.read_csv(double_agg_path, index_col=0)\n",
    "    df_large_test = pd.read_csv(large_test_path)\n",
    "\n",
    "    df_single_agg = df_single_agg.rename(columns={\"click\": \"nb_clicks\", \"c\": \"count\"})\n",
    "    df_double_agg = df_double_agg.rename(columns={\"click\": \"nb_clicks\", \"c\": \"count\"})\n",
    "\n",
    "    return X, y, X_test, y_test, df_single_agg, df_double_agg, df_large_test\n",
    "\n",
    "\n",
    "X, y, X_test, y_test, df_single_agg, df_double_agg, df_large_test = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd6f52",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42854f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:03:11.127911Z",
     "start_time": "2022-01-14T18:03:11.122197Z"
    }
   },
   "outputs": [],
   "source": [
    "LOGFILE = \"results_agg_gbt_clicks.log\"\n",
    "\n",
    "\n",
    "def print_and_log(x):\n",
    "    print(x)\n",
    "    with open(LOGFILE, \"a+\") as handle:\n",
    "        handle.write(x + \"\\n\")\n",
    "\n",
    "\n",
    "SEED = 2022\n",
    "\n",
    "# Set seed for reproducibility results\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "set_seed(2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f8a6e5",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0908c8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:03:11.149746Z",
     "start_time": "2022-01-14T18:03:11.129355Z"
    }
   },
   "outputs": [],
   "source": [
    "class AggClickEncoder:\n",
    "    \"\"\"\n",
    "    Beta Target Encoding.\n",
    "    Encode single and double features using a smoothed version of the observed CTR.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, single_agg_data, double_agg_data, prior_weight=100):\n",
    "        self.single_agg_data = single_agg_data\n",
    "        self.double_agg_data = double_agg_data\n",
    "        self.prior_weight = prior_weight\n",
    "        self.prior = 0\n",
    "        self.single_agg_posteriors = dict()\n",
    "        self.single_agg_count = dict()\n",
    "        self.double_agg_posteriors = dict()\n",
    "        self.double_agg_count = dict()\n",
    "\n",
    "    def _fit(self):\n",
    "        single_agg_posteriors = dict()\n",
    "        single_agg_count = dict()\n",
    "        double_agg_posteriors = dict()\n",
    "        double_agg_count = dict()\n",
    "\n",
    "        # Initialize dicts\n",
    "        for i in range(19):\n",
    "            single_agg_posteriors[i] = {}\n",
    "            single_agg_count[i] = {}\n",
    "            for j in range(i + 1, 19):\n",
    "                double_agg_posteriors[(i, j)] = {}\n",
    "                double_agg_count[(i, j)] = {}\n",
    "\n",
    "        # Compute prior CTR over feature 0\n",
    "        prior = (\n",
    "            self.single_agg_data[self.single_agg_data.feature_1_id == 0][\"nb_clicks\"].sum()\n",
    "            / self.single_agg_data[self.single_agg_data.feature_1_id == 0][\"count\"].sum()\n",
    "        )\n",
    "        self.prior = prior\n",
    "\n",
    "        # Compute posterior CTR per single & double feature modality, using a beta prior\n",
    "        for id, value, count, clicks in zip(\n",
    "            self.single_agg_data[\"feature_1_id\"].values,\n",
    "            self.single_agg_data[\"feature_1_value\"].values,\n",
    "            self.single_agg_data[\"count\"].values,\n",
    "            self.single_agg_data[\"nb_clicks\"].values,\n",
    "        ):\n",
    "            if count <= 0:\n",
    "                single_agg_posteriors[id][value] = np.nan\n",
    "                single_agg_count[id][value] = 0\n",
    "            else:\n",
    "                ctr = clicks / count\n",
    "                single_agg_posteriors[id][value] = (ctr * count + self.prior_weight * prior) / (\n",
    "                    count + self.prior_weight\n",
    "                )\n",
    "                single_agg_count[id][value] = count\n",
    "        del self.single_agg_data\n",
    "\n",
    "        for id_1, id_2, value_1, value_2, count, clicks in zip(\n",
    "            self.double_agg_data[\"feature_1_id\"].values,\n",
    "            self.double_agg_data[\"feature_2_id\"].values,\n",
    "            self.double_agg_data[\"feature_1_value\"].values,\n",
    "            self.double_agg_data[\"feature_2_value\"].values,\n",
    "            self.double_agg_data[\"count\"].values,\n",
    "            self.double_agg_data[\"nb_clicks\"].values,\n",
    "        ):\n",
    "            if count <= 0:\n",
    "                double_agg_posteriors[(id_1, id_2)][(value_1, value_2)] = np.nan\n",
    "                double_agg_count[(id_1, id_2)][(value_1, value_2)] = 0\n",
    "            else:\n",
    "                ctr = clicks / count\n",
    "                double_agg_posteriors[(id_1, id_2)][(value_1, value_2)] = (ctr * count + self.prior_weight * prior) / (\n",
    "                    count + self.prior_weight\n",
    "                )\n",
    "                double_agg_count[(id_1, id_2)][(value_1, value_2)] = count\n",
    "        del self.double_agg_data\n",
    "        return (\n",
    "            single_agg_posteriors,\n",
    "            single_agg_count,\n",
    "            double_agg_posteriors,\n",
    "            double_agg_count,\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(\"Fitting aggregated click encoder\")\n",
    "        (\n",
    "            self.single_agg_posteriors,\n",
    "            self.single_agg_count,\n",
    "            self.double_agg_posteriors,\n",
    "            self.double_agg_count,\n",
    "        ) = self._fit()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self._encode(X)\n",
    "        return X\n",
    "\n",
    "    def _encode(self, X):\n",
    "        n = 19\n",
    "        rows = X.shape[0]\n",
    "        for i in range(n):\n",
    "            ctr_results = np.zeros(rows, dtype=np.float32)\n",
    "            count_results = np.zeros(rows, dtype=np.float32)\n",
    "            values = X[f\"hash_{i}\"].values\n",
    "            for j in range(rows):\n",
    "                try:\n",
    "                    ctr_results[j] = self.single_agg_posteriors[i][values[j]]\n",
    "                    count_results[j] = self.single_agg_count[i][values[j]]\n",
    "                except KeyError:\n",
    "                    # Unseen modality\n",
    "                    ctr_results[j] = np.nan\n",
    "                    count_results[j] = 0\n",
    "            X[f\"feature_{i}_count\"] = count_results\n",
    "            X[f\"feature_{i}_ctr\"] = ctr_results\n",
    "\n",
    "        for i in range(n - 1):\n",
    "            for j in range(i + 1, n):\n",
    "                ctr_results = np.zeros(rows, dtype=np.float32)\n",
    "                count_results = np.zeros(rows, dtype=np.float32)\n",
    "                values = list(zip(X[f\"hash_{i}\"].values, X[f\"hash_{j}\"].values))\n",
    "                for k in range(rows):\n",
    "                    try:\n",
    "                        ctr_results[k] = self.double_agg_posteriors[(i, j)][values[k]]\n",
    "                        count_results[k] = self.double_agg_count[(i, j)][values[k]]\n",
    "                    except KeyError:\n",
    "                        # Unseen modality\n",
    "                        ctr_results[k] = np.nan\n",
    "                        count_results[k] = 0\n",
    "\n",
    "                X[f\"double_feature_{i}_{j}_count\"] = count_results\n",
    "                X[f\"double_feature_{i}_{j}_ctr\"] = ctr_results\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        X = self.transform(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class ColumnsSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encoder used to select only used columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns_to_drop=None, validate=False):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "        self.validate = validate\n",
    "        self.columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.columns_to_drop is not None:\n",
    "            self.columns = [col for col in X.columns if col not in self.columns_to_drop]\n",
    "        else:\n",
    "            self.columns = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        columns = [col for col in self.columns if col in X.columns]\n",
    "        if self.validate and len(columns) < len(self.columns):\n",
    "            missing = set(self.columns).difference(columns)\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "        return X[columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e329a59",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789bcfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:03:11.155849Z",
     "start_time": "2022-01-14T18:03:11.151398Z"
    }
   },
   "outputs": [],
   "source": [
    "def LLH(prediction, y):\n",
    "    llh = np.log(prediction) * y + np.log(1 - prediction) * (1 - y)\n",
    "    return sum(llh) / len(y)\n",
    "\n",
    "\n",
    "def Entropy(y):\n",
    "    py = sum(y > 0) / len(y)\n",
    "    return py * np.log(py) + (1 - py) * np.log(1 - py)\n",
    "\n",
    "\n",
    "def Nllh(prediction, y):\n",
    "    if any(prediction < 0) or any(prediction > 1):\n",
    "        return np.nan\n",
    "    h = Entropy(y)\n",
    "    llh = LLH(prediction, y)\n",
    "    return (h - llh) / h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814f11d",
   "metadata": {},
   "source": [
    "# Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff06953",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:03:11.166894Z",
     "start_time": "2022-01-14T18:03:11.157482Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(X_train, X_valid, y_train, y_valid, lgb_params):\n",
    "\n",
    "    train_dataset = lgb.Dataset(X_train, y_train)\n",
    "    valid_dataset = lgb.Dataset(X_valid, y_valid)\n",
    "    model = lgb.train(\n",
    "        params=lgb_params,\n",
    "        train_set=train_dataset,\n",
    "        valid_sets=[valid_dataset],\n",
    "        num_boost_round=50_000,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200,\n",
    "    )\n",
    "\n",
    "    best_iter = model.best_iteration\n",
    "    # Retrain on all the data\n",
    "    train_dataset = lgb.Dataset(\n",
    "        pd.concat([X_train, X_valid], axis=0, ignore_index=True),\n",
    "        pd.concat([y_train, y_valid], axis=0, ignore_index=True),\n",
    "    )\n",
    "    model = lgb.train(params=lgb_params, train_set=train_dataset, num_boost_round=best_iter)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run(X, y, X_test, y_test, df_single_agg, df_double_agg, gaussian_sigma=17, prior_weight=200):\n",
    "\n",
    "    # Create copies\n",
    "    df_single_agg_c = df_single_agg.copy()\n",
    "    df_double_agg_c = df_double_agg.copy()\n",
    "\n",
    "    # Adding noise\n",
    "    df_single_agg_c[\"count\"] = df_single_agg_c[\"count\"] + np.random.normal(0, gaussian_sigma, df_single_agg.shape[0])\n",
    "    df_single_agg_c[\"nb_clicks\"] = df_single_agg_c[\"nb_clicks\"] + np.random.normal(\n",
    "        0, gaussian_sigma, df_single_agg.shape[0]\n",
    "    )\n",
    "    df_double_agg_c[\"count\"] = df_double_agg_c[\"count\"] + np.random.normal(0, gaussian_sigma, df_double_agg.shape[0])\n",
    "    df_double_agg_c[\"nb_clicks\"] = df_double_agg_c[\"nb_clicks\"] + np.random.normal(\n",
    "        0, gaussian_sigma, df_double_agg.shape[0]\n",
    "    )\n",
    "\n",
    "    # Feature engineering pipeline\n",
    "    encoder_list = [AggClickEncoder(df_single_agg_c, df_double_agg_c, prior_weight=prior_weight)]\n",
    "    to_drop = [f\"hash_{i}\" for i in range(19)]\n",
    "    encoder_list.extend([ColumnsSelector(columns_to_drop=to_drop, validate=True)])\n",
    "    fe_pipeline = make_pipeline(*encoder_list)\n",
    "\n",
    "    X = fe_pipeline.fit_transform(X, y)\n",
    "    X_test = fe_pipeline.transform(X_test)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    lgb_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"random_state\": 42,\n",
    "        \"feature_fraction\": 0.7,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"deterministic\": True,\n",
    "        \"force_col_wise\": True,\n",
    "    }\n",
    "\n",
    "    model = train(X_train, X_valid, y_train, y_valid, lgb_params)\n",
    "\n",
    "    test_prediction = model.predict(X_test)\n",
    "    llh_test = Nllh(test_prediction, y_test)\n",
    "\n",
    "    return llh_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2fe04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T18:07:18.935263Z",
     "start_time": "2022-01-14T18:03:11.168693Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_and_log(\"Baseline\")\n",
    "sigma = 17\n",
    "prior_weight = 10\n",
    "\n",
    "llh_test = run(X, y, X_test, y_test, df_single_agg, df_double_agg, gaussian_sigma=sigma, prior_weight=prior_weight)\n",
    "tolog = f\"gaussianSigma:{sigma};priorWeight:{prior_weight};llh_test:{llh_test};\"\n",
    "print_and_log(tolog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6927a",
   "metadata": {},
   "source": [
    "# Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08789f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-14T21:31:49.218651Z",
     "start_time": "2022-01-14T18:07:18.936809Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_and_log(\"Benching noise robustness with prior weight\")\n",
    "\n",
    "# We assume that the function that maps a prior weight to a given sigma is monotonically increasing\n",
    "# For a given sigma, we will therefore only explore the space of priors weights greater than\n",
    "# the past weights used to regularize lower sigmas.\n",
    "\n",
    "sigmas = [0, 10, 17, 50, 250, 1_000, 5_000, 25_000, 100_000]\n",
    "prior_weights = [0, 10, 20, 50, 100, 200, 500, 1000, 5000, 10000]\n",
    "last_weight = 0\n",
    "for sigma in sigmas:\n",
    "    best_llh = 0\n",
    "    for prior_weight in prior_weights:\n",
    "        if prior_weight < last_weight:\n",
    "            continue\n",
    "        metrics = []\n",
    "        for i in range(5):\n",
    "            llh_test = run(\n",
    "                X,\n",
    "                y,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                df_single_agg,\n",
    "                df_double_agg,\n",
    "                gaussian_sigma=sigma,\n",
    "                prior_weight=prior_weight,\n",
    "            )\n",
    "            metrics.append(llh_test)\n",
    "        mean_llh = np.mean(metrics)\n",
    "        tolog = f\"gaussianSigma:{sigma};priorWeight:{prior_weight};llh_test:{mean_llh}+/-{np.std(metrics)};\"\n",
    "        print_and_log(tolog)\n",
    "        if mean_llh > best_llh:\n",
    "            best_llh = mean_llh\n",
    "            last_weight = prior_weight\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ce07c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-14T18:02:58.592Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_and_log(\"Benching performances as a function of number of available granular samples\")\n",
    "\n",
    "nb_of_samples = [1e5, 2e5, 5e5, 1e6, 2e6, df_large_test.shape[0]]\n",
    "sigma = 17\n",
    "prior_weight = 10\n",
    "\n",
    "for n in nb_of_samples:\n",
    "    metrics = []\n",
    "    for i in range(5):\n",
    "        df_tmp = df_large_test.sample(n=int(n), replace=False)\n",
    "        X_tmp, y_tmp = df_tmp.drop(columns=[\"click\", \"sale\"]), df_tmp[\"click\"]\n",
    "        llh_test = run(\n",
    "            X_tmp,\n",
    "            y_tmp,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            df_single_agg,\n",
    "            df_double_agg,\n",
    "            gaussian_sigma=sigma,\n",
    "            prior_weight=prior_weight,\n",
    "        )\n",
    "        metrics.append(llh_test)\n",
    "    tolog = f\"gaussianSigma:{sigma};priorWeight:{prior_weight}; nbOfSamples:{n};llh_test:{np.mean(metrics)}+/-{np.std(metrics)};\"\n",
    "    print_and_log(tolog)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "a36b085e9ff735ee0fad94699c928a54e5528d32a02652250964e3ed6cb11995"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python368jvsc74a57bd0a36b085e9ff735ee0fad94699c928a54e5528d32a02652250964e3ed6cb11995"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
