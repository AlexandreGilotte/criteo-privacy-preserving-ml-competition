{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f031ce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T14:52:59.530778Z",
     "start_time": "2021-08-30T14:52:28.858558Z"
    }
   },
   "source": [
    "This notebook read the large train set, and produce aggregated data similar to those provided in the challenge.\n",
    "Those aggregated data are then saved on disk to use as input of the ML algos.\n",
    "Unlike the challenge data however, we do not add Gaussian noise to the aggregated data; to allow comparing the results with different levels of noise. Remember that a Gaussian noise of sigma 17 should be added to get a dataset really similar to the one we provided in the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeefe999",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T15:30:01.136969Z",
     "start_time": "2021-08-30T15:30:00.868006Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2e1a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T15:30:01.942784Z",
     "start_time": "2021-08-30T15:30:01.939972Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = [\"click\", \"sale\"]\n",
    "allfeatures = [\"hash_\" + str(i) for i in range(0, 19)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d97c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T15:31:54.417574Z",
     "start_time": "2021-08-30T15:31:54.413873Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_on_features(features, mincount, filename):\n",
    "    df = pd.read_csv(filename, usecols=labels + features, dtype=np.int32)\n",
    "    df[\"c\"] = 1\n",
    "    df = df.groupby(features).sum().reset_index()\n",
    "    df = df[df.c > mincount].copy()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d00b92eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T15:31:55.201800Z",
     "start_time": "2021-08-30T15:31:55.196102Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_on_all_pairs(\n",
    "    allfeatures,\n",
    "    mincount=10,\n",
    "    filename=\"criteo-ppml-challenge-adkdd21-dataset-raw-granular-data.csv\",\n",
    "    gaussian_sigma=None,\n",
    "):\n",
    "    allpairsdf = pd.DataFrame()\n",
    "    for f0 in allfeatures:\n",
    "        feature_1_id = int(f0.split(\"_\")[-1])\n",
    "        for f1 in allfeatures:\n",
    "            feature_2_id = int(f1.split(\"_\")[-1])\n",
    "            if not feature_1_id < feature_2_id:\n",
    "                continue\n",
    "            print(\"aggregating on\", f0, f1)\n",
    "            features = [f0, f1]\n",
    "            df = aggregate_on_features(features, mincount, filename)\n",
    "            df[\"feature_1_id\"] = feature_1_id\n",
    "            df[\"feature_2_id\"] = feature_2_id\n",
    "            df = df.rename(\n",
    "                {\n",
    "                    features[0]: \"feature_1_value\",\n",
    "                    features[1]: \"feature_2_value\",\n",
    "                },\n",
    "                axis=1,\n",
    "            )\n",
    "            allpairsdf = pd.concat([allpairsdf, df])\n",
    "    if gaussian_sigma is not None:\n",
    "        allpairsdf[\"c\"] += np.random.normal(0, gaussian_sigma, len(allpairsdf))\n",
    "        allpairsdf[\"click\"] += np.random.normal(0, gaussian_sigma, len(allpairsdf))\n",
    "        allpairsdf[\"sale\"] += np.random.normal(0, gaussian_sigma, len(allpairsdf))\n",
    "    return allpairsdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c879c4cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T20:37:36.753812Z",
     "start_time": "2021-08-30T15:31:56.269116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating on hash_0 hash_1\n",
      "aggregating on hash_0 hash_2\n",
      "aggregating on hash_0 hash_3\n",
      "aggregating on hash_0 hash_4\n",
      "aggregating on hash_0 hash_5\n",
      "aggregating on hash_0 hash_6\n",
      "aggregating on hash_0 hash_7\n",
      "aggregating on hash_0 hash_8\n",
      "aggregating on hash_0 hash_9\n",
      "aggregating on hash_0 hash_10\n",
      "aggregating on hash_0 hash_11\n",
      "aggregating on hash_0 hash_12\n",
      "aggregating on hash_0 hash_13\n",
      "aggregating on hash_0 hash_14\n",
      "aggregating on hash_0 hash_15\n",
      "aggregating on hash_0 hash_16\n",
      "aggregating on hash_0 hash_17\n",
      "aggregating on hash_0 hash_18\n",
      "aggregating on hash_1 hash_2\n",
      "aggregating on hash_1 hash_3\n",
      "aggregating on hash_1 hash_4\n",
      "aggregating on hash_1 hash_5\n",
      "aggregating on hash_1 hash_6\n",
      "aggregating on hash_1 hash_7\n",
      "aggregating on hash_1 hash_8\n",
      "aggregating on hash_1 hash_9\n",
      "aggregating on hash_1 hash_10\n",
      "aggregating on hash_1 hash_11\n",
      "aggregating on hash_1 hash_12\n",
      "aggregating on hash_1 hash_13\n",
      "aggregating on hash_1 hash_14\n",
      "aggregating on hash_1 hash_15\n",
      "aggregating on hash_1 hash_16\n",
      "aggregating on hash_1 hash_17\n",
      "aggregating on hash_1 hash_18\n",
      "aggregating on hash_2 hash_3\n",
      "aggregating on hash_2 hash_4\n",
      "aggregating on hash_2 hash_5\n",
      "aggregating on hash_2 hash_6\n",
      "aggregating on hash_2 hash_7\n",
      "aggregating on hash_2 hash_8\n",
      "aggregating on hash_2 hash_9\n",
      "aggregating on hash_2 hash_10\n",
      "aggregating on hash_2 hash_11\n",
      "aggregating on hash_2 hash_12\n",
      "aggregating on hash_2 hash_13\n",
      "aggregating on hash_2 hash_14\n",
      "aggregating on hash_2 hash_15\n",
      "aggregating on hash_2 hash_16\n",
      "aggregating on hash_2 hash_17\n",
      "aggregating on hash_2 hash_18\n",
      "aggregating on hash_3 hash_4\n",
      "aggregating on hash_3 hash_5\n",
      "aggregating on hash_3 hash_6\n",
      "aggregating on hash_3 hash_7\n",
      "aggregating on hash_3 hash_8\n",
      "aggregating on hash_3 hash_9\n",
      "aggregating on hash_3 hash_10\n",
      "aggregating on hash_3 hash_11\n",
      "aggregating on hash_3 hash_12\n",
      "aggregating on hash_3 hash_13\n",
      "aggregating on hash_3 hash_14\n",
      "aggregating on hash_3 hash_15\n",
      "aggregating on hash_3 hash_16\n",
      "aggregating on hash_3 hash_17\n",
      "aggregating on hash_3 hash_18\n",
      "aggregating on hash_4 hash_5\n",
      "aggregating on hash_4 hash_6\n",
      "aggregating on hash_4 hash_7\n",
      "aggregating on hash_4 hash_8\n",
      "aggregating on hash_4 hash_9\n",
      "aggregating on hash_4 hash_10\n",
      "aggregating on hash_4 hash_11\n",
      "aggregating on hash_4 hash_12\n",
      "aggregating on hash_4 hash_13\n",
      "aggregating on hash_4 hash_14\n",
      "aggregating on hash_4 hash_15\n",
      "aggregating on hash_4 hash_16\n",
      "aggregating on hash_4 hash_17\n",
      "aggregating on hash_4 hash_18\n",
      "aggregating on hash_5 hash_6\n",
      "aggregating on hash_5 hash_7\n",
      "aggregating on hash_5 hash_8\n",
      "aggregating on hash_5 hash_9\n",
      "aggregating on hash_5 hash_10\n",
      "aggregating on hash_5 hash_11\n",
      "aggregating on hash_5 hash_12\n",
      "aggregating on hash_5 hash_13\n",
      "aggregating on hash_5 hash_14\n",
      "aggregating on hash_5 hash_15\n",
      "aggregating on hash_5 hash_16\n",
      "aggregating on hash_5 hash_17\n",
      "aggregating on hash_5 hash_18\n",
      "aggregating on hash_6 hash_7\n",
      "aggregating on hash_6 hash_8\n",
      "aggregating on hash_6 hash_9\n",
      "aggregating on hash_6 hash_10\n",
      "aggregating on hash_6 hash_11\n",
      "aggregating on hash_6 hash_12\n",
      "aggregating on hash_6 hash_13\n",
      "aggregating on hash_6 hash_14\n",
      "aggregating on hash_6 hash_15\n",
      "aggregating on hash_6 hash_16\n",
      "aggregating on hash_6 hash_17\n",
      "aggregating on hash_6 hash_18\n",
      "aggregating on hash_7 hash_8\n",
      "aggregating on hash_7 hash_9\n",
      "aggregating on hash_7 hash_10\n",
      "aggregating on hash_7 hash_11\n",
      "aggregating on hash_7 hash_12\n",
      "aggregating on hash_7 hash_13\n",
      "aggregating on hash_7 hash_14\n",
      "aggregating on hash_7 hash_15\n",
      "aggregating on hash_7 hash_16\n",
      "aggregating on hash_7 hash_17\n",
      "aggregating on hash_7 hash_18\n",
      "aggregating on hash_8 hash_9\n",
      "aggregating on hash_8 hash_10\n",
      "aggregating on hash_8 hash_11\n",
      "aggregating on hash_8 hash_12\n",
      "aggregating on hash_8 hash_13\n",
      "aggregating on hash_8 hash_14\n",
      "aggregating on hash_8 hash_15\n",
      "aggregating on hash_8 hash_16\n",
      "aggregating on hash_8 hash_17\n",
      "aggregating on hash_8 hash_18\n",
      "aggregating on hash_9 hash_10\n",
      "aggregating on hash_9 hash_11\n",
      "aggregating on hash_9 hash_12\n",
      "aggregating on hash_9 hash_13\n",
      "aggregating on hash_9 hash_14\n",
      "aggregating on hash_9 hash_15\n",
      "aggregating on hash_9 hash_16\n",
      "aggregating on hash_9 hash_17\n",
      "aggregating on hash_9 hash_18\n",
      "aggregating on hash_10 hash_11\n",
      "aggregating on hash_10 hash_12\n",
      "aggregating on hash_10 hash_13\n",
      "aggregating on hash_10 hash_14\n",
      "aggregating on hash_10 hash_15\n",
      "aggregating on hash_10 hash_16\n",
      "aggregating on hash_10 hash_17\n",
      "aggregating on hash_10 hash_18\n",
      "aggregating on hash_11 hash_12\n",
      "aggregating on hash_11 hash_13\n",
      "aggregating on hash_11 hash_14\n",
      "aggregating on hash_11 hash_15\n",
      "aggregating on hash_11 hash_16\n",
      "aggregating on hash_11 hash_17\n",
      "aggregating on hash_11 hash_18\n",
      "aggregating on hash_12 hash_13\n",
      "aggregating on hash_12 hash_14\n",
      "aggregating on hash_12 hash_15\n",
      "aggregating on hash_12 hash_16\n",
      "aggregating on hash_12 hash_17\n",
      "aggregating on hash_12 hash_18\n",
      "aggregating on hash_13 hash_14\n",
      "aggregating on hash_13 hash_15\n",
      "aggregating on hash_13 hash_16\n",
      "aggregating on hash_13 hash_17\n",
      "aggregating on hash_13 hash_18\n",
      "aggregating on hash_14 hash_15\n",
      "aggregating on hash_14 hash_16\n",
      "aggregating on hash_14 hash_17\n",
      "aggregating on hash_14 hash_18\n",
      "aggregating on hash_15 hash_16\n",
      "aggregating on hash_15 hash_17\n",
      "aggregating on hash_15 hash_18\n",
      "aggregating on hash_16 hash_17\n",
      "aggregating on hash_16 hash_18\n",
      "aggregating on hash_17 hash_18\n"
     ]
    }
   ],
   "source": [
    "## This may take severals hours:\n",
    "# - there are 19*18/2 = 171 pairs of features to process,\n",
    "# - each of them requires to read the full csv and run in a few minutes\n",
    "# Memory requirement is about 10 Go (Mostly from pandas overhead)\n",
    "allpairsdf = aggregate_on_all_pairs(allfeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a63edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T20:38:09.743081Z",
     "start_time": "2021-08-30T20:37:36.755656Z"
    }
   },
   "outputs": [],
   "source": [
    "allpairsdf.to_csv(\"aggregated_pairs.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f63b35e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:03:14.676007Z",
     "start_time": "2021-08-30T21:03:14.670911Z"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_on_all_single(\n",
    "    allfeatures, mincount=0, filename=\"criteo-ppml-challenge-adkdd21-dataset-raw-granular-data.csv\", gaussianSigma=None\n",
    "):\n",
    "    allpairsdf = pd.DataFrame()\n",
    "    for f0 in allfeatures:\n",
    "        print(\"aggregating on\", f0)\n",
    "\n",
    "        features = [f0]\n",
    "        df = aggregate_on_features(features, mincount, filename)\n",
    "        df[\"feature_1_id\"] = int(f0.split(\"_\")[-1])\n",
    "        df = df.rename({features[0]: \"feature_1_value\"}, axis=1)\n",
    "        allpairsdf = pd.concat([allpairsdf, df])\n",
    "    if gaussianSigma is not None:\n",
    "        allpairsdf[\"c\"] += np.random.normal(0, gaussianSigma, len(allpairsdf))\n",
    "        allpairsdf[\"click\"] += np.random.normal(0, gaussianSigma, len(allpairsdf))\n",
    "        allpairsdf[\"sale\"] += np.random.normal(0, gaussianSigma, len(allpairsdf))\n",
    "    return allpairsdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2baf46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:34:46.446482Z",
     "start_time": "2021-08-30T21:03:17.458027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating on hash_0\n",
      "aggregating on hash_1\n",
      "aggregating on hash_2\n",
      "aggregating on hash_3\n",
      "aggregating on hash_4\n",
      "aggregating on hash_5\n",
      "aggregating on hash_6\n",
      "aggregating on hash_7\n",
      "aggregating on hash_8\n",
      "aggregating on hash_9\n",
      "aggregating on hash_10\n",
      "aggregating on hash_11\n",
      "aggregating on hash_12\n",
      "aggregating on hash_13\n",
      "aggregating on hash_14\n",
      "aggregating on hash_15\n",
      "aggregating on hash_16\n",
      "aggregating on hash_17\n",
      "aggregating on hash_18\n"
     ]
    }
   ],
   "source": [
    "allsingles = aggregate_on_all_single(allfeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116a4fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-30T21:34:48.318613Z",
     "start_time": "2021-08-30T21:34:46.448014Z"
    }
   },
   "outputs": [],
   "source": [
    "allsingles.to_csv(\"aggregated_singles.csv\", index=True)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "interpreter": {
   "hash": "a36b085e9ff735ee0fad94699c928a54e5528d32a02652250964e3ed6cb11995"
  },
  "kernelspec": {
   "display_name": "Python Criteo (MOAB #43678)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
